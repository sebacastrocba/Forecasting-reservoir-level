{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, RegressorMixin\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.imputation import DropMissingData\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.timeseries.forecasting import (\n",
    "    LagFeatures,\n",
    "    WindowFeatures,\n",
    ")\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "#from hybrid_regressor import CombinedRegressor, LinearBoost\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "from mizani.breaks import date_breaks\n",
    "from plotnine import *\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_embalse</th>\n",
       "      <th>cota</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 05:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 06:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 07:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 08:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28 11:00:00+00:00</th>\n",
       "      <td>RAPEL</td>\n",
       "      <td>103.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28 12:00:00+00:00</th>\n",
       "      <td>RAPEL</td>\n",
       "      <td>103.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28 13:00:00+00:00</th>\n",
       "      <td>RAPEL</td>\n",
       "      <td>103.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28 14:00:00+00:00</th>\n",
       "      <td>RAPEL</td>\n",
       "      <td>103.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-28 15:00:00+00:00</th>\n",
       "      <td>RAPEL</td>\n",
       "      <td>103.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113040 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          nombre_embalse    cota\n",
       "timestamp                                       \n",
       "2022-01-01 04:00:00+00:00      ANGOSTURA  316.42\n",
       "2022-01-01 05:00:00+00:00      ANGOSTURA  316.42\n",
       "2022-01-01 06:00:00+00:00      ANGOSTURA  316.44\n",
       "2022-01-01 07:00:00+00:00      ANGOSTURA  316.45\n",
       "2022-01-01 08:00:00+00:00      ANGOSTURA  316.46\n",
       "...                                  ...     ...\n",
       "2023-01-28 11:00:00+00:00          RAPEL  103.72\n",
       "2023-01-28 12:00:00+00:00          RAPEL  103.72\n",
       "2023-01-28 13:00:00+00:00          RAPEL  103.72\n",
       "2023-01-28 14:00:00+00:00          RAPEL  103.72\n",
       "2023-01-28 15:00:00+00:00          RAPEL  103.72\n",
       "\n",
       "[113040 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservoir_hourly = pd.read_csv('../data/processed/reservoir_data_hourly.csv', parse_dates=['timestamp'])\n",
    "reservoir_hourly = reservoir_hourly.set_index('timestamp')\n",
    "reservoir_hourly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection, removal and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, value_col, period = None, robust = True):\n",
    "    serie = df[value_col]\n",
    "    res = STL(serie, period = period, robust = robust).fit()\n",
    "    resid = res.resid\n",
    "    q1 = resid.quantile(0.25)\n",
    "    q3 = resid.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (3*iqr)\n",
    "    upper = q3 + (3*iqr)\n",
    "\n",
    "    anomalies = serie[(resid < lower) | (resid >= upper)]\n",
    "    df = df.assign(anomaly = np.where(df[value_col].index.isin(anomalies.index), 1, 0))\n",
    "    df[\"value_corrected\"] = np.where(df[\"anomaly\"] == True, np.NaN, df[value_col])\n",
    "    df.interpolate(method = \"linear\", inplace=True)\n",
    "    df[\"value_corrected\"] = np.where(df[\"value_corrected\"].isna(), df[value_col], df[\"value_corrected\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_list = reservoir_hourly[\"nombre_embalse\"].unique()\n",
    "\n",
    "emb_df_list = list()\n",
    "emb_df_anomalies_list = list()\n",
    "for emb in reservoir_list:\n",
    "    emb_df = reservoir_hourly[reservoir_hourly[\"nombre_embalse\"] == emb]\n",
    "    emb_df = emb_df.asfreq(\"H\")\n",
    "    emb_df = emb_df.sort_index()\n",
    "    \n",
    "    emb_df_sin_outliers = detect_outliers(emb_df, \"cota\", robust=True)\n",
    "    emb_df_list.append(emb_df_sin_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_embalse</th>\n",
       "      <th>cota</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>value_corrected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.42</td>\n",
       "      <td>0</td>\n",
       "      <td>316.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 05:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.42</td>\n",
       "      <td>0</td>\n",
       "      <td>316.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 06:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.44</td>\n",
       "      <td>0</td>\n",
       "      <td>316.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 07:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.45</td>\n",
       "      <td>0</td>\n",
       "      <td>316.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 08:00:00+00:00</th>\n",
       "      <td>ANGOSTURA</td>\n",
       "      <td>316.46</td>\n",
       "      <td>0</td>\n",
       "      <td>316.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          nombre_embalse    cota  anomaly  value_corrected\n",
       "timestamp                                                                 \n",
       "2022-01-01 04:00:00+00:00      ANGOSTURA  316.42        0           316.42\n",
       "2022-01-01 05:00:00+00:00      ANGOSTURA  316.42        0           316.42\n",
       "2022-01-01 06:00:00+00:00      ANGOSTURA  316.44        0           316.44\n",
       "2022-01-01 07:00:00+00:00      ANGOSTURA  316.45        0           316.45\n",
       "2022-01-01 08:00:00+00:00      ANGOSTURA  316.46        0           316.46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned = pd.concat(emb_df_list, axis = 0)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split\n",
    "val_len = 24 # one day\n",
    "in_sample_df = data_cleaned.groupby(\"nombre_embalse\", group_keys=False).apply(lambda x : x.iloc[:-val_len, :])\n",
    "out_of_sample_df = data_cleaned.groupby(\"nombre_embalse\", group_keys=False).apply(lambda x : x.iloc[-val_len:, :])\n",
    "\n",
    "in_sample_df = in_sample_df.reset_index()\n",
    "out_of_sample_df = out_of_sample_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "test_time = pd.Timedelta(24*7*4, unit = \"H\")\n",
    "split_point = in_sample_df[\"timestamp\"].max() - test_time\n",
    "\n",
    "X_train = in_sample_df[in_sample_df[\"timestamp\"] < split_point]\n",
    "X_test = in_sample_df[in_sample_df[\"timestamp\"] >= split_point - pd.Timedelta(24*4, unit = \"H\")]\n",
    "\n",
    "y_train = in_sample_df[in_sample_df[\"timestamp\"] < split_point][[\"timestamp\",\"nombre_embalse\",\"value_corrected\"]]\n",
    "y_test = in_sample_df[in_sample_df[\"timestamp\"] >= split_point - pd.Timedelta(24*4, unit = \"H\")][[\"timestamp\", \"nombre_embalse\", \"value_corrected\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.set_index([\"timestamp\", \"nombre_embalse\"])\n",
    "X_test = X_test.set_index([\"timestamp\", \"nombre_embalse\"])\n",
    "y_train = y_train.set_index([\"timestamp\", \"nombre_embalse\"])\n",
    "y_test = y_test.set_index([\"timestamp\", \"nombre_embalse\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Features Class\n",
    "class AddFourierFeatures(BaseEstimator, TransformerMixin):\n",
    "    seconds_per_day = 24*60*60     # Daily dataset\n",
    "    seconds_per_hour = 60*60       # Hourly dataset\n",
    "\n",
    "    def __init__(self, K, periods: list, by = \"day\"):\n",
    "        self.K = K\n",
    "        self.periods = periods\n",
    "        self.by = by\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        dates = X.index\n",
    "        \n",
    "        for period in self.periods:\n",
    "            term = self.K / period\n",
    "            timestamps = dates.map(datetime.datetime.timestamp)\n",
    "            ts_scaled = []\n",
    "\n",
    "            for ts in timestamps:\n",
    "                if self.by == \"day\":\n",
    "                    x_scaled = round(ts / self.seconds_per_day)\n",
    "                    ts_scaled.append(x_scaled)\n",
    "                else:\n",
    "                    x_scaled = round(ts / self.seconds_per_hour)\n",
    "                    ts_scaled.append(x_scaled)\n",
    "\n",
    "            X[\"fourier_sin\"] = [np.sin(2 * np.pi * term * ts) for ts in ts_scaled]\n",
    "            X[\"fourier_cos\"] = [np.cos(2 * np.pi * term * ts) for ts in ts_scaled]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "horizon = 24\n",
    "\n",
    "dtf = DatetimeFeatures(\n",
    "    variables=\"index\",\n",
    "    features_to_extract=[\n",
    "        \"hour\",\n",
    "        \"day_of_month\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "        \"day_of_year\",\n",
    "        \"week\",\n",
    "        \"day_of_week\",\n",
    "        \"weekend\"\n",
    "    ],\n",
    "    drop_original = False,\n",
    "    utc = True\n",
    ")\n",
    "\n",
    "cyclicf = CyclicalFeatures(\n",
    "    variables=[\"hour\", \"month\", \"day_of_year\"],\n",
    "    drop_original= True\n",
    ")\n",
    "\n",
    "fourierf = AddFourierFeatures(\n",
    "    K = 1,\n",
    "    periods=[horizon, horizon*2],\n",
    "    by = \"hour\"\n",
    ")\n",
    "\n",
    "lagf = LagFeatures(\n",
    "    variables=\"value_corrected\",\n",
    "    periods=list(range(1,horizon+1)),\n",
    "    missing_values = \"ignore\"\n",
    ")\n",
    "\n",
    "windf24 = WindowFeatures(\n",
    "    variables=\"value_corrected\",\n",
    "    functions=[\"mean\"],\n",
    "    window=[int(horizon/2), horizon],\n",
    "    freq=\"1H\",\n",
    "    missing_values=\"ignore\"\n",
    ")\n",
    "\n",
    "imputer = DropMissingData()\n",
    "\n",
    "drop_features = DropFeatures(features_to_drop=[\"value_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pipeline = Pipeline([\n",
    "    (\"datetime features\", dtf),\n",
    "    (\"cyclical features\", cyclicf),\n",
    "    (\"fourier features\", fourierf),\n",
    "    (\"lag features\", lagf),\n",
    "    (\"window features 24H\", windf24),\n",
    "    (\"imputer\", imputer),\n",
    "    (\"drop features\", drop_features)\n",
    "])\n",
    "\n",
    "centrales = X_train.index.get_level_values(1).unique()\n",
    "X_train_t_list = list()\n",
    "X_test_t_list = list()\n",
    "for cen in centrales:\n",
    "    emb_df = X_train[X_train.index.get_level_values(1) == cen]\n",
    "    emb_df = emb_df.reset_index(level=1)\n",
    "    emb_df_t = prep_pipeline.fit_transform(emb_df)\n",
    "    emb_df_t = emb_df_t.set_index(\"nombre_embalse\", append=True)\n",
    "    X_train_t_list.append(emb_df_t)\n",
    "    \n",
    "    emb_test = X_test[X_test.index.get_level_values(1) == cen]\n",
    "    emb_test = emb_test.reset_index(level=1)\n",
    "    emb_test_t = prep_pipeline.transform(emb_test)\n",
    "    emb_test_t = emb_test_t.set_index(\"nombre_embalse\", append=True)\n",
    "    X_test_t_list.append(emb_test_t)\n",
    "\n",
    "X_train_t = pd.concat(X_train_t_list, axis = 0)\n",
    "X_test_t = pd.concat(X_test_t_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align\n",
    "y_train_t = y_train.loc[X_train_t.index]\n",
    "y_test_t = y_test.loc[X_test_t.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train_t.reset_index(level=1)\n",
    "X_train_t[\"nombre_embalse\"] = X_train_t[\"nombre_embalse\"].astype(\"category\")\n",
    "X_test_t = X_test_t.reset_index(level=1)\n",
    "X_test_t[\"nombre_embalse\"] = X_test_t[\"nombre_embalse\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [var for var in X_train_t.columns if ((X_train_t[var].dtype == 'float') | (X_train_t[var].dtype == 'int')) & (var != \"value_corrected\")]\n",
    "categorical_features = [var for var in X_train_t.columns if ((X_train_t[var].dtype == 'O') | (X_train_t[var].dtype == 'category'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nombre_embalse']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features\n",
    "scaler = SklearnTransformerWrapper(transformer=StandardScaler(),\n",
    "                                   variables=numeric_features)\n",
    "\n",
    "encoder = OneHotEncoder(variables=categorical_features)\n",
    "\n",
    "transformer_pipe = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"encoder\", encoder)\n",
    "])\n",
    "\n",
    "X_train_prep = transformer_pipe.fit_transform(X_train_t)\n",
    "X_test_prep = transformer_pipe.transform(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104388, 53), (104388, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep.shape, y_train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Building Custom Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m## Credits -> \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCombinedRegressor\u001b[39;00m(BaseEstimator, RegressorMixin):\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m      5\u001b[0m                  base_regressor\u001b[39m=\u001b[39mlgbm\u001b[39m.\u001b[39mLGBMRegressor, \n\u001b[1;32m      6\u001b[0m                  backup_regressor\u001b[39m=\u001b[39mLinearRegression, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                  random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                  \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_regressor \u001b[39m=\u001b[39m base_regressor()\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mCombinedRegressor\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCombinedRegressor\u001b[39;00m(BaseEstimator, RegressorMixin):\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m      5\u001b[0m                  base_regressor\u001b[39m=\u001b[39mlgbm\u001b[39m.\u001b[39mLGBMRegressor, \n\u001b[0;32m----> 6\u001b[0m                  backup_regressor\u001b[39m=\u001b[39mLinearRegression, \n\u001b[1;32m      7\u001b[0m                  lower\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, \n\u001b[1;32m      8\u001b[0m                  upper\u001b[39m=\u001b[39m\u001b[39m1.9\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                  \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_regressor \u001b[39m=\u001b[39m base_regressor()\n\u001b[1;32m     12\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackup_regressor \u001b[39m=\u001b[39m backup_regressor()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Building Custom Model\n",
    "## Credits -> \n",
    "class CombinedRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, \n",
    "                 base_regressor=lgbm.LGBMRegressor, \n",
    "                 backup_regressor=LinearRegression, \n",
    "                 lower=0.1, \n",
    "                 upper=1.9,\n",
    "                 random_state=None,\n",
    "                 **kwargs):\n",
    "        self.base_regressor = base_regressor()\n",
    "        self.backup_regressor = backup_regressor()\n",
    "        \n",
    "        self.set_random_state(random_state)\n",
    "        \n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        \n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.base_regressor.fit(X, y)\n",
    "        self.backup_regressor.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        y_base = self.base_regressor.predict(X)\n",
    "        y_base = y_base.reshape(-1,1)\n",
    "        y_backup = self.backup_regressor.predict(X)\n",
    "        y_pred = np.where((self.lower * y_backup <= y_base) & (y_base <= self.upper * y_backup), \n",
    "                          y_base,\n",
    "                          y_backup)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # not as good as sklearn pretty printing,\n",
    "        # but shows updated params of subestimator\n",
    "        return f'CombinedRegressor({self.get_params()})'\n",
    "    \n",
    "    def get_params(self, deep=False, **kwargs):\n",
    "        base_regressor_params = self.base_regressor.get_params(**kwargs)\n",
    "        # remove random state as it should be a global param of the estimator\n",
    "        base_regressor_params.pop('random_state', None)\n",
    "        base_regressor_params = {'base_regressor__' + key: value \n",
    "                                 for key, value \n",
    "                                 in base_regressor_params.items()}\n",
    "        \n",
    "        backup_regressor_params = self.backup_regressor.get_params(**kwargs)\n",
    "        backup_regressor_params.pop('random_state', None)\n",
    "        backup_regressor_params = {'backup_regressor__' + key: value \n",
    "                                   for key, value \n",
    "                                   in backup_regressor_params.items()}\n",
    "        \n",
    "        own_params = {\n",
    "            'lower': self.lower,\n",
    "            'upper': self.upper,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        params = {**own_params,\n",
    "                  **base_regressor_params, \n",
    "                  **backup_regressor_params, \n",
    "                 }\n",
    "        \n",
    "        if deep:\n",
    "            params['base_regressor'] = self.base_regressor\n",
    "            params['backup_regressor'] = self.backup_regressor\n",
    "        return params\n",
    "    \n",
    "    def set_random_state(self, value):\n",
    "        self.random_state = value\n",
    "        if 'random_state' in self.base_regressor.get_params().keys():\n",
    "            self.base_regressor.set_params(random_state=value)\n",
    "        # linear reg does not have random state, but just in case..\n",
    "        if 'random_state' in self.backup_regressor.get_params().keys():\n",
    "            self.backup_regressor.set_params(random_state=value)\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if key.startswith('base_regressor__'):\n",
    "                trunc_key = {key[len('base_regressor__'):]: value}\n",
    "                self.base_regressor.set_params(**trunc_key)\n",
    "            elif key.startswith('backup_regressor__'):\n",
    "                trunc_key = {key[len('backup_regressor__'):]: value}\n",
    "                self.backup_regressor.set_params(**trunc_key)\n",
    "            elif key == 'random_state':\n",
    "                self.set_random_state(value)\n",
    "            else:\n",
    "                # try to fetch old value first to raise AttributeError\n",
    "                # if not exists\n",
    "                old_value = getattr(self, key)\n",
    "                setattr(self, key, value)\n",
    "        # set_params needs to return self to make gridsearch work\n",
    "        return self\n",
    "        \n",
    "    def _more_tags(self):\n",
    "        # no_validation added because validation is happening \n",
    "        # within built-in sklearn estimators\n",
    "        return {**self.base_regressor._more_tags(), 'no_validation': True}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reservoir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7029e492f706b46c67808507ecd16dab067c027a9d2a27caa4dd225e273ec307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
